{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import array\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import numpy as np\n",
    "#import scipy.sparse as sps\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel, NaiveBayes\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#textfile=sc.textFile('hdfs://0.0.0.0:9000/user/weatherAUS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_location = \"hdfs://0.0.0.0:9000/user/weatherAUS.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option('header','true').option('inferSchema','true').csv(file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Date: string, Location: string, MinTemp: string, MaxTemp: string, Rainfall: string, Evaporation: string, Sunshine: string, WindGustDir: string, WindGustSpeed: string, WindDir9am: string, WindDir3pm: string, WindSpeed9am: string, WindSpeed3pm: string, Humidity9am: string, Humidity3pm: string, Pressure9am: string, Pressure3pm: string, Cloud9am: string, Cloud3pm: string, Temp9am: string, Temp3pm: string, RainToday: string, RISK_MM: double, RainTomorrow: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      Date|count|\n",
      "+----------+-----+\n",
      "|2016-07-06|   49|\n",
      "|2013-03-14|   49|\n",
      "|2014-03-17|   49|\n",
      "|2016-08-17|   49|\n",
      "|2017-05-14|   49|\n",
      "|2016-09-11|   49|\n",
      "|2014-05-27|   49|\n",
      "|2017-05-11|   49|\n",
      "|2014-07-14|   49|\n",
      "|2016-08-08|   49|\n",
      "+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+------------+-----+\n",
      "|    Location|count|\n",
      "+------------+-----+\n",
      "|    Canberra| 3418|\n",
      "|      Sydney| 3337|\n",
      "|       Perth| 3193|\n",
      "|      Darwin| 3192|\n",
      "|      Hobart| 3188|\n",
      "|    Brisbane| 3161|\n",
      "|    Adelaide| 3090|\n",
      "|     Bendigo| 3034|\n",
      "|  Townsville| 3033|\n",
      "|AliceSprings| 3031|\n",
      "+------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------+-----+\n",
      "|MinTemp|count|\n",
      "+-------+-----+\n",
      "|    9.6|  883|\n",
      "|     11|  883|\n",
      "|   10.2|  880|\n",
      "|   10.5|  867|\n",
      "|   10.8|  860|\n",
      "|      9|  853|\n",
      "|     12|  850|\n",
      "|     10|  849|\n",
      "|     13|  844|\n",
      "|   10.4|  842|\n",
      "+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------+-----+\n",
      "|MaxTemp|count|\n",
      "+-------+-----+\n",
      "|     20|  871|\n",
      "|   19.8|  829|\n",
      "|     19|  827|\n",
      "|   20.4|  820|\n",
      "|   20.8|  804|\n",
      "|   19.9|  803|\n",
      "|   19.5|  801|\n",
      "|     21|  799|\n",
      "|   18.5|  793|\n",
      "|   18.2|  792|\n",
      "+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------+-----+\n",
      "|Rainfall|count|\n",
      "+--------+-----+\n",
      "|       0|90275|\n",
      "|     0.2| 8685|\n",
      "|     0.4| 3750|\n",
      "|     0.6| 2562|\n",
      "|     0.8| 2028|\n",
      "|       1| 1747|\n",
      "|     1.2| 1515|\n",
      "|      NA| 1406|\n",
      "|     1.4| 1365|\n",
      "|     1.6| 1187|\n",
      "+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----------+-----+\n",
      "|Evaporation|count|\n",
      "+-----------+-----+\n",
      "|         NA|60843|\n",
      "|          4| 3282|\n",
      "|          8| 2574|\n",
      "|        2.2| 2057|\n",
      "|          2| 1996|\n",
      "|        2.6| 1975|\n",
      "|        2.4| 1963|\n",
      "|        1.8| 1945|\n",
      "|          3| 1937|\n",
      "|        3.4| 1934|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------+-----+\n",
      "|Sunshine|count|\n",
      "+--------+-----+\n",
      "|      NA|67816|\n",
      "|       0| 2308|\n",
      "|    10.7| 1087|\n",
      "|      11| 1078|\n",
      "|    10.8| 1058|\n",
      "|    10.5| 1018|\n",
      "|    10.9| 1013|\n",
      "|    10.3|  999|\n",
      "|    10.2|  985|\n",
      "|      10|  973|\n",
      "+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----------+-----+\n",
      "|WindGustDir|count|\n",
      "+-----------+-----+\n",
      "|          W| 9780|\n",
      "|         NA| 9330|\n",
      "|         SE| 9309|\n",
      "|          E| 9071|\n",
      "|          N| 9033|\n",
      "|        SSE| 8993|\n",
      "|          S| 8949|\n",
      "|        WSW| 8901|\n",
      "|         SW| 8797|\n",
      "|        SSW| 8610|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------------+-----+\n",
      "|WindGustSpeed|count|\n",
      "+-------------+-----+\n",
      "|           NA| 9270|\n",
      "|           35| 9070|\n",
      "|           39| 8656|\n",
      "|           31| 8310|\n",
      "|           37| 7903|\n",
      "|           33| 7814|\n",
      "|           41| 7236|\n",
      "|           30| 6943|\n",
      "|           43| 6513|\n",
      "|           28| 6382|\n",
      "+-------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----------+-----+\n",
      "|WindDir9am|count|\n",
      "+----------+-----+\n",
      "|         N|11393|\n",
      "|        NA|10013|\n",
      "|        SE| 9162|\n",
      "|         E| 9024|\n",
      "|       SSE| 8966|\n",
      "|        NW| 8552|\n",
      "|         S| 8493|\n",
      "|         W| 8260|\n",
      "|        SW| 8237|\n",
      "|       NNE| 7948|\n",
      "+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----------+-----+\n",
      "|WindDir3pm|count|\n",
      "+----------+-----+\n",
      "|        SE|10663|\n",
      "|         W| 9911|\n",
      "|         S| 9598|\n",
      "|       WSW| 9329|\n",
      "|        SW| 9182|\n",
      "|       SSE| 9142|\n",
      "|         N| 8667|\n",
      "|       WNW| 8656|\n",
      "|        NW| 8468|\n",
      "|       ESE| 8382|\n",
      "+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+------------+-----+\n",
      "|WindSpeed9am|count|\n",
      "+------------+-----+\n",
      "|           9|13400|\n",
      "|          13|12851|\n",
      "|          11|11514|\n",
      "|          17|10599|\n",
      "|           7|10587|\n",
      "|          15|10396|\n",
      "|           6| 8989|\n",
      "|           0| 8612|\n",
      "|          19| 8579|\n",
      "|          20| 7904|\n",
      "+------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+------------+-----+\n",
      "|WindSpeed3pm|count|\n",
      "+------------+-----+\n",
      "|          13|12338|\n",
      "|          17|12306|\n",
      "|          20|11504|\n",
      "|          15|11301|\n",
      "|          19|11034|\n",
      "|          11| 9844|\n",
      "|           9| 9577|\n",
      "|          24| 8846|\n",
      "|          22| 8410|\n",
      "|          28| 6395|\n",
      "+------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----------+-----+\n",
      "|Humidity9am|count|\n",
      "+-----------+-----+\n",
      "|         99| 3350|\n",
      "|         70| 2985|\n",
      "|         69| 2962|\n",
      "|         68| 2961|\n",
      "|         65| 2952|\n",
      "|         71| 2939|\n",
      "|         66| 2916|\n",
      "|         67| 2895|\n",
      "|         64| 2867|\n",
      "|         75| 2859|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----------+-----+\n",
      "|Humidity3pm|count|\n",
      "+-----------+-----+\n",
      "|         NA| 3610|\n",
      "|         52| 2699|\n",
      "|         55| 2685|\n",
      "|         57| 2679|\n",
      "|         53| 2650|\n",
      "|         59| 2639|\n",
      "|         58| 2604|\n",
      "|         54| 2595|\n",
      "|         51| 2577|\n",
      "|         56| 2574|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----------+-----+\n",
      "|Pressure9am|count|\n",
      "+-----------+-----+\n",
      "|         NA|14014|\n",
      "|     1016.4|  804|\n",
      "|     1017.9|  779|\n",
      "|     1018.7|  764|\n",
      "|       1018|  761|\n",
      "|     1015.9|  757|\n",
      "|     1017.3|  756|\n",
      "|     1017.8|  755|\n",
      "|     1016.3|  753|\n",
      "|     1017.2|  745|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----------+-----+\n",
      "|Pressure3pm|count|\n",
      "+-----------+-----+\n",
      "|         NA|13981|\n",
      "|     1015.5|  773|\n",
      "|     1015.3|  767|\n",
      "|     1015.7|  763|\n",
      "|     1015.6|  761|\n",
      "|     1015.1|  752|\n",
      "|     1013.5|  751|\n",
      "|     1015.8|  751|\n",
      "|     1015.4|  745|\n",
      "|       1016|  738|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------+-----+\n",
      "|Cloud9am|count|\n",
      "+--------+-----+\n",
      "|      NA|53657|\n",
      "|       7|19749|\n",
      "|       1|15558|\n",
      "|       8|14389|\n",
      "|       0| 8587|\n",
      "|       6| 8072|\n",
      "|       2| 6442|\n",
      "|       3| 5854|\n",
      "|       5| 5510|\n",
      "|       4| 4373|\n",
      "+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------+-----+\n",
      "|Cloud3pm|count|\n",
      "+--------+-----+\n",
      "|      NA|57094|\n",
      "|       7|18052|\n",
      "|       1|14827|\n",
      "|       8|12407|\n",
      "|       6| 8869|\n",
      "|       2| 7153|\n",
      "|       3| 6836|\n",
      "|       5| 6743|\n",
      "|       4| 5254|\n",
      "|       0| 4957|\n",
      "+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------+-----+\n",
      "|Temp9am|count|\n",
      "+-------+-----+\n",
      "|     NA|  904|\n",
      "|     17|  901|\n",
      "|   13.8|  887|\n",
      "|   14.8|  873|\n",
      "|     16|  869|\n",
      "|   16.6|  855|\n",
      "|     14|  855|\n",
      "|     15|  852|\n",
      "|   16.5|  844|\n",
      "|     13|  831|\n",
      "+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------+-----+\n",
      "|Temp3pm|count|\n",
      "+-------+-----+\n",
      "|     NA| 2726|\n",
      "|     20|  871|\n",
      "|     19|  858|\n",
      "|   18.5|  856|\n",
      "|   18.4|  856|\n",
      "|   17.8|  845|\n",
      "|   19.2|  826|\n",
      "|   19.4|  825|\n",
      "|     18|  821|\n",
      "|   19.3|  821|\n",
      "+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+---------+------+\n",
      "|RainToday| count|\n",
      "+---------+------+\n",
      "|       No|109332|\n",
      "|      Yes| 31455|\n",
      "|       NA|  1406|\n",
      "+---------+------+\n",
      "\n",
      "+-------+-----+\n",
      "|RISK_MM|count|\n",
      "+-------+-----+\n",
      "|    0.0|91077|\n",
      "|    0.2| 8762|\n",
      "|    0.4| 3781|\n",
      "|    0.6| 2591|\n",
      "|    0.8| 2055|\n",
      "|    1.0| 1761|\n",
      "|    1.2| 1535|\n",
      "|    1.4| 1379|\n",
      "|    1.6| 1201|\n",
      "|    1.8| 1104|\n",
      "+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+------------+------+\n",
      "|RainTomorrow| count|\n",
      "+------------+------+\n",
      "|          No|110316|\n",
      "|         Yes| 31877|\n",
      "+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in df.columns:\n",
    "  \n",
    "  df.groupBy(i).count().orderBy(\"count\", ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df=df.drop('Date','Location','Evaporation','Sunshine','Cloud9am',\"Cloud3pm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "def transform_cat_to_num(col,data):\n",
    "  \n",
    "  indexer = StringIndexer(inputCol=col, outputCol=col+\"Index\") \n",
    "  indexed = indexer.fit(data).transform(data)\n",
    "  return indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaning=transform_cat_to_num(\"WindGustDir\",data_df)\n",
    "data_cleaning=transform_cat_to_num(\"WindDir9am\",data_cleaning)\n",
    "data_cleaning=transform_cat_to_num(\"WindDir3pm\",data_cleaning)\n",
    "data_cleaning=transform_cat_to_num(\"RainToday\",data_cleaning)\n",
    "data_cleaning=transform_cat_to_num(\"RainTomorrow\",data_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=data_cleaning.where(\"RainTodayIndex!=2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=d.drop(\"WindGustDir\",\"WindDir9am\",\"WindDir3pm\",\"RainToday\",\"RainTomorrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode,regexp_extract,col\n",
    "d=d.filter(col('Rainfall') != 'NA')\n",
    "d=d.filter(col('windSpeed9am') != 'NA')\n",
    "d=d.filter(col('windSpeed3pm') != 'NA')\n",
    "d=d.filter(col('humidity3pm') != 'NA')\n",
    "d=d.filter(col('pressure9am') != 'NA')\n",
    "d=d.filter(col('pressure3pm') != 'NA')\n",
    "d=d.filter(col('temp9am') != 'NA')\n",
    "d=d.filter(col('temp3pm') != 'NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124218"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "def change_type(d):\n",
    "    for i in d.columns:\n",
    "        d = d.withColumn(i, d[i].cast(DoubleType()))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dFrame=change_type(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDF=dFrame.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119590"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd=newDF.rdd.map(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_0=rdd.filter(lambda line:line[-1]==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93403"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_0.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_1=rdd.filter(lambda line:line[-1]==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26187"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ploting num target classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Container object of 2 artists>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5de877d3d0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x=[\"0\",\"1\"]\n",
    "y=[target_0.count(),target_1.count()]\n",
    "plt.bar(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_label(line):\n",
    "    return int(line[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_nb(line):\n",
    "    \n",
    "    l = []\n",
    "    for i in line[:17]:\n",
    "        if i == \"NA\":\n",
    "            l=l+[0]\n",
    "        elif float(i) < 0:\n",
    "            l=l+[0]\n",
    "        else:\n",
    "            l=l+[i]\n",
    "    return np.array(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def extract_features_dt(line):\n",
    "    \n",
    "    l = []\n",
    "    for i in line[:17]:\n",
    "        if i != \"NA\":\n",
    "            l=l+[i]\n",
    "        else:\n",
    "            l=l+[0]\n",
    "    return np.array(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a view or table\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "data_feat_lab_dt=rdd.map(lambda line:LabeledPoint(extract_label(line),extract_features_dt(line)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(0.0, [13.4,22.9,0.6,44.0,20.0,24.0,71.0,22.0,1007.7,1007.1,16.9,21.8,0.0,0.0,7.0,7.0,0.0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_feat_lab_dt.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_dt, test_dt) = data_feat_lab_dt.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel, NaiveBayes\n",
    "LogReg=LogisticRegressionWithSGD.train(training_dt,iterations=10, regParam=0.03)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsAndPreds1 = training_dt.map(lambda p: (p.label, LogReg.predict(p.features)))\n",
    "trainAcc = labelsAndPreds1.filter(lambda x:x[0]==x[1]).count() / float(training_dt.count())\n",
    "print(\"Training acc for LR = \" + str(trainAcc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsAndPreds1 = test_dt.map(lambda p: (p.label, LogReg.predict(p.features)))\n",
    "testAcc = labelsAndPreds1.filter(lambda x:x[0]==x[1]).count() / float(test_dt.count())\n",
    "print(\"Test acc for LR = \" + str(testAcc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning_logReg_rate(training,test,learning_rate):\n",
    "    n\n",
    "    for i in range(100,1001):\n",
    "        if i%100==0:\n",
    "            \n",
    "            (training_dt, test_dt) = data_feat_lab_dt.randomSplit([0.8, 0.2])\n",
    "             LogReg=LogisticRegressionWithSGD.train(training,iterations=i, regParam=learning_rate)\n",
    "            labelsAndPreds1 = training.map(lambda p: (p.label, LogReg.predict(p.features)))\n",
    "            trainErr1 = labelsAndPreds1.filter(lambda x:x[0]==x[1]).count() / float(training.count())\n",
    "            print(\"Training acc for LR for iteration \"+str(i/10)+\" is \" +str(trainErr1))\n",
    "            labelsAndPreds1 = test.map(lambda p: (p.label, LogReg.predict(p.features)))\n",
    "            testErr1 = labelsAndPreds1.filter(lambda x:x[0]==x[1]).count() / float(test.count())\n",
    "            print(\"Test acc for LR for iteration \"+str(i/10)+\" is \" +str(testErr1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_logReg_rate(training_dt,test_dt,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_feat_lab_nb=rdd.map(lambda line:LabeledPoint(extract_label(line),extract_features_nb(line)))\n",
    "(train_data_nb,test_data_nb)= data_feat_lab_nb.randomSplit([0.8, 0.2])\n",
    "nbModel = NaiveBayes.train(train_data_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labelsAndPreds1 = train_data_nb.map(lambda p: (p.label, nbModel.predict(p.features)))\n",
    "trainErr1 = labelsAndPreds1.filter(lambda x:x[0]==x[1]).count() / float(train_data_nb.count())\n",
    "print(\"Training acc for LR = \" + str(trainErr1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labelsAndPreds1 = test_data_nb.map(lambda p: (p.label, nbModel.predict(p.features)))\n",
    "testErr1 = labelsAndPreds1.filter(lambda x:x[0]==x[1]).count() / float(test_data_nb.count())\n",
    "print(\"Test acc for LR = \" + str(testErr1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "model = DecisionTree.trainClassifier(training_dt, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     impurity='gini', maxDepth=5, maxBins=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_dt.map(lambda x: x.features))\n",
    "labelsAndPredictions = test_dt.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(\n",
    "    lambda lp: lp[0] == lp[1]).count() / float(test_dt.count())\n",
    "print('Test Accuracy decision tree = ' + str(testErr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[MinTemp: double, MaxTemp: double, Humidity9am: double, Humidity3pm: double, Pressure3pm: double, Temp9am: double, Temp3pm: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x=newDF.select('MinTemp','MaxTemp','Humidity9am','Humidity3pm','Pressure3pm','Temp9am',\"Temp3pm\")\n",
    "display(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_rg=x.rdd.map(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_label_rg(line):\n",
    "    return int(line[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def extract_features_rg(line):\n",
    "    \n",
    "    l = []\n",
    "    for i in line[:6]:\n",
    "        if i != \"NA\":\n",
    "            l=l+[i]\n",
    "        else:\n",
    "            l=l+[0]\n",
    "    return np.array(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13.4, 22.9, 71.0, 22.0, 1007.1, 16.9, 21.8]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_rg.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg=rdd_rg.map(lambda line:extract_features_rg(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import StandardScaler, StandardScalerModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.util import MLUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler1 = StandardScaler().fit(rdd_rg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = (scaler1.transform(rdd_rg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([2.1044, 3.2777, 3.7116, 1.0632, 143.4736, 2.6204, 3.1905])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_feat_lab_rg=data1.map(lambda line:LabeledPoint(extract_label_rg(line),extract_features_rg(line)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(3.0, [2.10442434736,3.27771215697,3.71158873387,1.06322387427,143.473602918,2.62038996989])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_feat_lab_rg.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_rg, test_rg) = data_feat_lab_rg.randomSplit([0.8, 0.2])\n",
    "from pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD\n",
    "LRmodel = LinearRegressionWithSGD.train(train_rg, iterations=100, step=0.00000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valuesAndPreds = test_rg.map(lambda p: (p.label, LRmodel.predict(p.features)))\n",
    "MSE = valuesAndPreds \\\n",
    "    .map(lambda vp: (vp[0] - vp[1])**2) \\\n",
    "    .reduce(lambda x, y: x + y) / valuesAndPreds.count()\n",
    "print(\"Mean Squared Error = \" + str(MSE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=LRmodel.predict(test_rg.map(lambda x:x.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rg.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LRTmodel = DecisionTree.trainRegressor(train_rg, categoricalFeaturesInfo={},\n",
    "                                    impurity='variance', maxDepth=5, maxBins=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = LRTmodel.predict(test_rg.map(lambda x: x.features))\n",
    "labelsAndPredictions = test_rg.map(lambda lp: lp.label).zip(predictions)\n",
    "testMSE = labelsAndPredictions.map(lambda lp: (lp[0] - lp[1]) * (lp[0] - lp[1])).sum() /\\\n",
    "    float(test_rg.count())\n",
    "print('Test Mean Squared Error = ' + str(testMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=LRTmodel.predict(test_rg.map(lambda x:x.features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rg.take(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.take(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from math import sqrt\n",
    "\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "display(newDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def extract_features_cl(line):\n",
    "    \n",
    "    l = []\n",
    "    for i in line[:16]:\n",
    "        if i != \"NA\":\n",
    "            l=l+[i]\n",
    "        else:\n",
    "            l=l+[0]\n",
    "    return np.array(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datacl=rdd.map(lambda line:extract_features_cl(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "clusters = KMeans.train(datacl, 2, maxIterations=100, initializationMode=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(point):\n",
    "    center = clusters.centers[clusters.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "WSSSE = datacl.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_pred=clusters.predict(datacl.map(lambda x:x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_cl = StandardScaler().fit(datacl)\n",
    "data1_cl = (scaler_cl.transform(datacl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_cl.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "clusters = KMeans.train(data1_cl, 2, maxIterations=100, initializationMode=\"random\")\n",
    "\n",
    "def error(point):\n",
    "    center = clusters.centers[clusters.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "WSSSE = data1_cl.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuning_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(1,20):\n",
    "    clusters = KMeans.train(data1_cl, k, maxIterations=100, initializationMode=\"random\")\n",
    "    WSSSE = data1_cl.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "    print(\"K= \" +str(k))\n",
    "    print(\"Within Set Sum of Squared Error = \" + str(WSSSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datacl_new=rdd_rg.map(lambda line:extract_features_cl(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "clusters = KMeans.train(datacl_new, 2, maxIterations=100, initializationMode=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(point):\n",
    "    center = clusters.centers[clusters.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "WSSSE = datacl_new.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
